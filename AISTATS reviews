1. I find the assumption that the offline dataset is generated using the uniform behavior policy quite weird. The motivation for offline RL is to make use of historical data without online exploration. In practice, you would simply never excute a uniform behavior policy ever.

2. It should be remarked that Assumption 3,4 are non-standard assumptions in RL and have been shown to be unnecessary for learning in block MDPs in general. They seem to be technical conditions so that one can directly borrow results from the classical clustering literature, which I'm not familiar with. In particular, assumption 3 (ii) (even the relaxed assumption in Appendix K) seems particularly strong, as it essentially eliminate the need for exploration -- the main challenge of RL, and ensures that a uniform action is enough to sufficiently explore the whole state space. With such an assumption there is no real difference between online learning and the generative model setting.
Can the authors please elaborate on whether these assumptions are essential for the proposed algorithm to work and why? Are they necessary assumptions to get the fast rate of $n\log(1/\epsilon)$? Personally, I've never seen a minimax rate of $log(1/\epsilon)$ in stochastic online learning tasks. Some intuition on what assumptions buy us this fast rate would be helpful.
