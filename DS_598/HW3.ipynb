{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67CoRHqXVctX"
   },
   "source": [
    "# Homework 3: Exploration in RL (due 11:59PM, April 5th, 2024)\n",
    "\n",
    "In this homework, you will implement and compare a number of exploration strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (2 points)\n",
    "\n",
    "Implement a **combination lock MDP** of length H = 20, number of actions A = 10, number of states per step = 3. Denote the states as $\\{s_{h;1}, s_{h;2}, s_{h;3}\\}_{h=1:20}$. While initializing the MDP, uniformly randomly choose one out of ten actions for each stage as the **good action**, denoted as $\\{a^*_h\\}_{h=1:20}$. Define the transition as: \n",
    "\n",
    "$$P(s_{h+1;1}|s_{h;1},a^*_h)=P(s_{h+1;2}|s_{h;1},a^*_h)=P(s_{h+1;1}|s_{h;2},a^*_h)=P(s_{h+1;2}|s_{h;2},a^*_h)=0.5$$\n",
    "while for $a\\neq a^*_h$,\n",
    "$$P(s_{h+1;3}|s_{h;1},a)=P(s_{h+1;3}|s_{h;2},a)=P(s_{h+1;3}|s_{h;3},a)=1$$\n",
    "Lastly, if you are in the bad state, for any action $a$,\n",
    "$$P(s_{h+1;3}|s_{h;3},a)=1$$\n",
    "In terms of rewards, the agent receives a reward of 10 upon reaching $s_{H,1}$ or $s_{H,2}$, at which point the episode will also reset. Otherwise the reward will always be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h_dSFrzHVctZ",
    "outputId": "1fa71d66-2132-4052-e42b-5169455f8ea7"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 14 (2457257028.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    def reset(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 14\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CombLockMDP(gym.Env):\n",
    "    \"\"\"\n",
    "    Methods:\n",
    "        reset(): Resets the environment to the starting state.\n",
    "        step(action): Takes a step in the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, H=10, A=10, S=3, R=10, seed=0):\n",
    "        ## Fill in here\n",
    "        ## use the seed input as the random seed when generating the good actions.\n",
    "        \n",
    "        self.H=H\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        ## Fill in here\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        ## Fill in here\n",
    "        \n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yTjoCWGVcta"
   },
   "source": [
    "## Problem 2 (2 points)\n",
    "\n",
    "Implement the UCBVI algorithm with reward bonus $b_h(s,a)=\\alpha\\sqrt{1/N_h(s,a)}$, where $\\alpha$ is a hyperparameter that you will tune.\n",
    "\n",
    "Run UCBVI in the combination lock MDP (with turned $\\alpha$) and plot its learning curve: Cumulative reward v.s. training episode same as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBVI(object):\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        ## fill in other local parameters here\n",
    "\n",
    "    def update(self, state, action, next_state, reward, done):\n",
    "        ## fill in the local updates of UCBVI, including counts, \\hat{P} and reward bonus and value iteration, \n",
    "        ## for which you can keep track of a Q table Q_h(s,a).\n",
    "\n",
    "    def action(self, state):\n",
    "        ## fill in the excuted action, i.e. return argmax_a \\hat{Q}(s,a)      \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: use a large enough num_episodes that make the algorithm converge.\n",
    "num_episodes = 1000000\n",
    "\n",
    "# Create an instance of the environment\n",
    "env = CombLockMDP()\n",
    "\n",
    "# Create an instance of UCBVI\n",
    "# remember to tune your alpha!!!\n",
    "\n",
    "agent = UCBVI(alpha) \n",
    "\n",
    "total_rewards = np.zeros(num_episodes)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    for h in range(env.H):\n",
    "        action = agent.action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_rewards[i_episode] += reward\n",
    "        agent.update(state,action,next_state,reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.title('Training Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgvWmycHVcta"
   },
   "source": [
    "## Problem 3 (2 points)\n",
    "\n",
    "Implement DQN with $\\epsilon$-greedy, where $\\epsilon$ is a hyperparameter that you will tune. (You should have already have this code from HW2!)\n",
    "\n",
    "Run DQN+$\\epsilon$-greedy in the combination lock MDP (with turned $\\epsilon$) and plot its learning curve: Cumulative reward v.s. training episode same as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NYntq8NVctb"
   },
   "outputs": [],
   "source": [
    "## Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_R3hhOMVctb"
   },
   "source": [
    "## Problem 4 (3 points)\n",
    "\n",
    "Implement DQN with RND using a randomly neural network as the target network $f:S\\rightarrow \\mathbb{R}^d$, and use the reward bonus in the form of $\\alpha||\\hat{f}(s)-f(s)||_2^2$. \n",
    "\n",
    "Run DQN+RND in the combination lock MDP (with turned hyperparameters, including $\\alpha$, the replay buffer size, learning rates, etc) and plot its learning curve: Cumulative reward v.s. training episode same as in HW2.\n",
    "\n",
    "An example implementation of DQN+RND can be found [here](https://github.com/orrivlin/MountainCar_DQN_RND)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElbxmHlGVctb"
   },
   "outputs": [],
   "source": [
    "## Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2_XsjYMVctc",
    "outputId": "2868c485-83d3-4f16-c8f3-78c0d0ee28dc"
   },
   "source": [
    "## Problem 5 (1 points)\n",
    "\n",
    "Plot the performance curve of UCBVI, DQN with $\\epsilon$-greedy, and DQN+RND in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
